{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-zTLHrFCT6KY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Aim is to predict the marks of students of the test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "p0KHq8ZgTpU4",
        "outputId": "9a436b0f-4f44-4d77-9f5d-fdda06614880"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    internet sex  traveltime  studytime  freetime  absences  age   iq  marks\n",
            "0         no   M           2          2         5         6   18  118  73.13\n",
            "1        yes   M           2          2         2         7   19  107  67.78\n",
            "2        yes   M           3          3         3         5   18  108  70.84\n",
            "3         no   F           3          3         4         9   17  100  66.68\n",
            "4        yes   M           1          4         3         4   19   96  71.88\n",
            "..       ...  ..         ...        ...       ...       ...  ...  ...    ...\n",
            "995      yes   M           2          1         1         9   18  108  64.12\n",
            "996       no   M           1          2         2         2   19  109  75.47\n",
            "997      yes   M           2          2         1         5   17  123  76.73\n",
            "998      yes   F           1          3         3         8   17  104  69.07\n",
            "999      yes   M           1          2         2         6   18  128  78.73\n",
            "\n",
            "[1000 rows x 9 columns]\n"
          ]
        }
      ],
      "source": [
        "# Use the file namd 'training data' to train the model\n",
        "\n",
        "data = pd.read_excel('Training data.xlsx')\n",
        "x_train = np.array(data.iloc[:,0:8])\n",
        "y_train = np.array(data.iloc[:,8]).reshape(-1,1)\n",
        "\n",
        "# Try plotting y_train with different features\n",
        "# To get an idea whether to add some features or not\n",
        "# Add some features if required in x_train\n",
        "\n",
        "# Also do label encoding for features not represented in numbers\n",
        "# refer the link if not know : https://youtu.be/589nCGeWG1w?si=t2Wa7LgbUOO4RooM\n",
        "print(data)\n",
        "def feature_changing(x_train):\n",
        "  x_train[x_train[:,1] == 'F', 1] = 0\n",
        "  x_train[x_train[:,1] == 'M', 1] = 1\n",
        "  x_train[x_train[:,0] == 'no', 0] = 0\n",
        "  x_train[x_train[:,0] == 'yes', 0] = 1\n",
        "  return x_train\n",
        "\n",
        "x_train = feature_changing(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "tYshvtYlVour"
      },
      "outputs": [],
      "source": [
        "def z_score(x_train):\n",
        "\n",
        "  # ---------\n",
        "    # write the code for feature scaling here\n",
        "    # Your code here\n",
        "    x_std=np.std(x_train.astype(float),axis=0)\n",
        "    x_mean=np.mean(x_train.astype(float),axis=0)\n",
        "  # ---------\n",
        "\n",
        "    return x_train,x_std,x_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AtrxEjTzJZ76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[ 0.46569947  0.49403542  0.89968661  0.92281959  1.26827402  2.66289298\n",
            "  0.63289731 11.63337114]\n",
            "[ 0.46569947  0.49403542  0.89968661  0.92281959  1.26827402  2.66289298\n",
            "  0.63289731 11.63337114]\n"
          ]
        }
      ],
      "source": [
        "x_train,x_std,x_mean = z_score(x_train)\n",
        "x_train = (x_train - x_mean)/x_std\n",
        "print(x_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "O5dOwbNbWJWa"
      },
      "outputs": [],
      "source": [
        "def cost(x_train,y_train,w,b):\n",
        "  pred=np.dot(x_train,w)+b\n",
        "  loss=np.mean((pred-y_train)**2)\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "hW8p2cTNU74W"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(x_train,y_train,w,b):\n",
        "  # pred=np.dot(x_train,w)+b\n",
        "  # delta=y_train-pred\n",
        "  # dw=np.dot(x_train.T,delta)/len(y_train)\n",
        "  # db=np.sum(delta)/len(y_train)\n",
        "  # learning_rate=0.01\n",
        "  # w=w-learning_rate*dw\n",
        "  # b=b-learning_rate*db\n",
        "  pred = np.dot(x_train, w) + b\n",
        "  delta = y_train - pred\n",
        "  dw =(x_train.T @ delta) / len(y_train)\n",
        "  db = np.sum(delta) / len(y_train)\n",
        "  learning_rate = 0.001\n",
        "\n",
        "  return dw,db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[-1.94037336]\n",
            " [-1.38258367]\n",
            " [-0.49627542]\n",
            " [ 3.50139822]\n",
            " [-0.81652037]\n",
            " [-0.71648011]\n",
            " [-0.72202511]\n",
            " [ 5.03791887]]\n"
          ]
        }
      ],
      "source": [
        "w = np.random.randn(x_train.shape[1],1)\n",
        "b = np.random.randn(1)\n",
        "dw,db=gradient_descent(x_train,y_train,w,b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "Kl-fioJ5WkYn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[73.13       73.13171114]\n",
            " [67.78       67.77583151]\n",
            " [70.84       70.8365458 ]\n",
            " [66.68       66.67867761]\n",
            " [71.88       71.87971797]\n",
            " [73.84       73.83617474]\n",
            " [71.47       71.46757392]\n",
            " [74.48       74.48385919]\n",
            " [73.48       73.48257792]\n",
            " [91.44       91.43491228]\n",
            " [79.38       79.37408001]\n",
            " [62.51       62.51153781]\n",
            " [75.         75.00135872]\n",
            " [62.64       62.64032369]\n",
            " [53.95       53.94739236]\n",
            " [68.26       68.26114875]\n",
            " [67.65       67.64828393]\n",
            " [76.44       76.43951275]\n",
            " [77.44       77.43630554]\n",
            " [78.45       78.45101148]\n",
            " [77.23       77.22904353]\n",
            " [70.31       70.30793049]\n",
            " [80.         79.99764653]\n",
            " [69.9        69.89641189]\n",
            " [65.42       65.41951807]\n",
            " [67.38       67.37566314]\n",
            " [73.17       73.17307521]\n",
            " [81.11       81.10754442]\n",
            " [82.87       82.87197069]\n",
            " [72.4        72.39629485]\n",
            " [65.         64.99663713]\n",
            " [75.49       75.48982008]\n",
            " [73.74       73.73972839]\n",
            " [68.4        68.39557831]\n",
            " [64.06       64.06620649]\n",
            " [79.62       79.62187347]\n",
            " [60.55       60.54587586]\n",
            " [79.06       79.06464726]\n",
            " [61.99       61.99402218]\n",
            " [77.3        77.2969447 ]\n",
            " [81.22       81.2227281 ]\n",
            " [67.43       67.42537729]\n",
            " [72.16       72.15977794]\n",
            " [67.11       67.11081753]\n",
            " [67.78       67.78072904]\n",
            " [62.99       62.99388345]\n",
            " [65.14       65.13587237]\n",
            " [56.96       56.96184833]\n",
            " [73.75       73.74756197]\n",
            " [64.25       64.25736524]\n",
            " [71.61       71.61042061]\n",
            " [66.72       66.72708654]\n",
            " [61.73       61.72642979]\n",
            " [75.25       75.25583713]\n",
            " [80.37       80.36404083]\n",
            " [80.59       80.59695254]\n",
            " [67.13       67.13176028]\n",
            " [75.13       75.1314579 ]\n",
            " [78.27       78.26470355]\n",
            " [58.17       58.16898925]\n",
            " [76.88       76.87983656]\n",
            " [62.38       62.37584491]\n",
            " [63.09       63.08550256]\n",
            " [73.21       73.21328056]\n",
            " [72.03       72.02792422]\n",
            " [66.38       66.38220513]\n",
            " [78.91       78.91095991]\n",
            " [76.35       76.35166612]\n",
            " [65.27       65.2720293 ]\n",
            " [80.82       80.82088372]\n",
            " [60.83       60.82694702]\n",
            " [65.23       65.23229454]\n",
            " [74.39       74.39084848]\n",
            " [71.04       71.04080239]\n",
            " [68.27       68.2723754 ]\n",
            " [81.3        81.29568491]\n",
            " [72.11       72.10569635]\n",
            " [69.73       69.73585635]\n",
            " [63.11       63.10645878]\n",
            " [65.88       65.877671  ]\n",
            " [61.36       61.36235247]\n",
            " [74.8        74.79881666]\n",
            " [72.86       72.86113273]\n",
            " [69.6        69.6019441 ]\n",
            " [67.93       67.92684294]\n",
            " [76.35       76.35165804]\n",
            " [66.52       66.51710673]\n",
            " [72.81       72.81537073]\n",
            " [74.36       74.35551166]\n",
            " [73.42       73.4170827 ]\n",
            " [78.12       78.12588088]\n",
            " [66.41       66.40759635]\n",
            " [66.98       66.98585017]\n",
            " [67.36       67.36081521]\n",
            " [65.7        65.70501047]\n",
            " [78.4        78.39765506]\n",
            " [76.09       76.09284776]\n",
            " [74.08       74.08140707]\n",
            " [55.33       55.33051734]\n",
            " [66.71       66.70526338]]\n",
            "[[74.29805689 74.59      ]\n",
            " [64.81028623 65.28      ]\n",
            " [79.00831028 79.24      ]\n",
            " [62.0851106  62.66      ]\n",
            " [67.56929415 67.83      ]\n",
            " [75.07420354 75.43      ]\n",
            " [76.85732313 77.06      ]\n",
            " [75.76061232 75.94      ]\n",
            " [72.32730128 72.51      ]\n",
            " [71.20062856 71.61      ]\n",
            " [64.50762671 65.03      ]\n",
            " [58.47813587 59.09      ]\n",
            " [69.71044192 70.        ]\n",
            " [58.31518663 59.06      ]\n",
            " [69.71714704 70.12      ]\n",
            " [74.12100703 74.39      ]\n",
            " [57.01286532 57.71      ]\n",
            " [79.24045517 79.34      ]\n",
            " [56.83418169 57.53      ]\n",
            " [60.24983817 60.87      ]\n",
            " [74.44954513 74.7       ]\n",
            " [63.05212963 63.7       ]\n",
            " [79.29490717 79.42      ]\n",
            " [64.03756251 64.52      ]\n",
            " [79.40681043 79.52      ]\n",
            " [85.41229164 85.41      ]\n",
            " [67.71852616 68.14      ]\n",
            " [67.80992857 68.28      ]\n",
            " [61.74568332 62.31      ]\n",
            " [82.34406382 82.25      ]\n",
            " [60.57424502 61.25      ]\n",
            " [68.7007405  69.06      ]\n",
            " [78.35302825 78.41      ]\n",
            " [75.64751844 75.8       ]\n",
            " [80.10723277 80.11      ]\n",
            " [69.35891399 69.74      ]\n",
            " [76.98220004 77.03      ]\n",
            " [75.03981026 75.4       ]\n",
            " [78.26770959 78.28      ]\n",
            " [68.24168161 68.64      ]\n",
            " [69.05508169 69.31      ]\n",
            " [61.26383947 61.77      ]\n",
            " [77.53943889 77.7       ]\n",
            " [74.40755491 74.53      ]\n",
            " [67.24778592 67.64      ]\n",
            " [63.86686047 64.56      ]\n",
            " [65.6418673  66.14      ]\n",
            " [71.66036958 71.91      ]\n",
            " [74.90913425 75.15      ]\n",
            " [73.76813726 74.1       ]\n",
            " [61.91785112 62.4       ]\n",
            " [63.97286187 64.43      ]\n",
            " [75.62585039 75.73      ]\n",
            " [71.2111624  71.52      ]\n",
            " [80.9568994  80.92      ]\n",
            " [61.04394925 61.66      ]\n",
            " [71.69995456 71.93      ]\n",
            " [85.35397077 85.24      ]\n",
            " [70.03300681 70.31      ]\n",
            " [79.17994785 79.31      ]\n",
            " [69.19651189 69.65      ]\n",
            " [70.08037246 70.62      ]\n",
            " [68.48341122 68.92      ]\n",
            " [75.7046956  75.74      ]\n",
            " [62.10967515 62.7       ]\n",
            " [74.29430306 74.38      ]\n",
            " [63.5459172  64.18      ]\n",
            " [64.85397626 65.25      ]\n",
            " [63.21993625 63.89      ]\n",
            " [74.46311046 74.55      ]\n",
            " [67.86224944 68.26      ]\n",
            " [76.47691462 76.65      ]\n",
            " [62.04149877 62.56      ]\n",
            " [57.6145561  58.36      ]\n",
            " [67.63615879 67.91      ]\n",
            " [65.34102422 65.81      ]\n",
            " [62.85781437 63.33      ]\n",
            " [63.75247827 64.3       ]\n",
            " [54.91664112 55.68      ]\n",
            " [72.16850611 72.48      ]\n",
            " [73.91456102 74.13      ]\n",
            " [64.7794555  65.27      ]\n",
            " [57.62670801 58.4       ]\n",
            " [75.89334245 76.02      ]\n",
            " [71.23502184 71.63      ]\n",
            " [68.44511395 68.84      ]\n",
            " [60.50738037 61.17      ]\n",
            " [68.3906864  68.72      ]\n",
            " [59.42177817 60.04      ]\n",
            " [59.47899326 60.18      ]\n",
            " [79.61475694 79.69      ]\n",
            " [72.44359028 72.6       ]\n",
            " [74.52190026 74.78      ]\n",
            " [74.9824718  75.17      ]\n",
            " [75.6607739  75.78      ]\n",
            " [80.82248528 80.77      ]\n",
            " [83.86913712 83.91      ]\n",
            " [78.43165065 78.67      ]\n",
            " [67.41599555 67.94      ]\n",
            " [58.63613995 59.27      ]]\n",
            "Optimization required, your accuracy is 72.5%\n"
          ]
        }
      ],
      "source": [
        "x_train = x_train.astype(np.float64)\n",
        "x_train,x_std,x_mean = z_score(x_train)\n",
        "\n",
        "np.random.seed(2147483647)\n",
        "w = np.random.randn(x_train.shape[1],1)\n",
        "b = np.random.randn(1)\n",
        "\n",
        "old_cost = 0\n",
        "\n",
        "while abs(old_cost - cost(x_train,y_train,w,b))>0.000001:\n",
        "  old_cost = cost(x_train,y_train,w,b)\n",
        "  dw,db = gradient_descent(x_train,y_train,w,b)\n",
        "  w = w + 1*dw\n",
        "  b = b + 1*db\n",
        "  \n",
        "out=np.zeros((100,2))\n",
        "out[:,0]=y_train[:100].T\n",
        "out[:,1]=(np.dot(x_train,w)+b)[:100].T\n",
        "print(out)\n",
        "x_predict = pd.read_excel('Test data.xlsx').iloc[:,:8].to_numpy()\n",
        "x_predict = feature_changing(x_predict)\n",
        "x_predict,x_std,x_mean = z_score(x_predict)\n",
        "x_predict = (x_predict - x_mean)/x_std\n",
        "ans = pd.read_excel('Test data.xlsx').iloc[:,8].to_numpy()\n",
        "\n",
        "y_predict = np.dot(x_predict,w) + b\n",
        "out=np.zeros((100,2))\n",
        "out[:,0]=y_predict[:100].T\n",
        "out[:,1]=ans[:100].T\n",
        "print(out)\n",
        "accuracy = 0\n",
        "for dim in range(len(ans)):\n",
        "  if abs(y_predict[dim]-ans[dim])<0.5:\n",
        "    accuracy += 1\n",
        "accuracy = round(accuracy*100/200.0,2)\n",
        "ok = 'Congratulations' if accuracy>95 else 'Optimization required'\n",
        "print(f\"{ok}, your accuracy is {accuracy}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
